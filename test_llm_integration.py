#!/usr/bin/env python3
# LLM Provider í†µí•© í…ŒìŠ¤íŠ¸

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.ai.llm_providers import get_llm_provider, LLMProviderFactory
from modules.ai.pydantic_models import TopicSelection, BlogContentResponse
from config import load_accounts
from modules.utils import logger

def test_llm_providers():
    """LLM Provider í†µí•© í…ŒìŠ¤íŠ¸"""
    print("=== LLM Provider í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===\n")
    
    # ê³„ì • ì •ë³´ ë¡œë“œ
    accounts = load_accounts()
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì • ì„¸íŠ¸:")
    for set_name, account_info in accounts.items():
        llm_config = account_info.get('llm', {})
        provider_type = llm_config.get('provider', 'default')
        model = llm_config.get('model', 'default')
        print(f"  - {set_name}: {provider_type} ({model})")
    print()
    
    # ê° ê³„ì • ì„¸íŠ¸ë³„ LLM Provider í…ŒìŠ¤íŠ¸
    for set_name in accounts.keys():
        print(f"=== {set_name} ê³„ì • í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # LLM Provider ìƒì„±
            llm_provider = get_llm_provider(set_name, accounts)
            
            print(f"Provider íƒ€ì…: {type(llm_provider).__name__}")
            print(f"ì‚¬ìš© ê°€ëŠ¥: {llm_provider.is_available()}")
            
            if llm_provider.is_available():
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
                test_messages = [
                    {
                        "role": "user",
                        "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤. 'í…ŒìŠ¤íŠ¸ ì™„ë£Œ'ë¼ê³ ë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
                    }
                ]
                
                print("í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡ ì¤‘...")
                response = llm_provider.generate(
                    messages=test_messages,
                    system_prompt="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                    max_tokens=100,
                    temperature=0
                )
                
                if response:
                    print(f"âœ… ì‘ë‹µ ì„±ê³µ: {response[:100]}{'...' if len(response) > 100 else ''}")
                else:
                    print("âŒ ì‘ë‹µ ì‹¤íŒ¨")
            else:
                print("âš ï¸ Provider ì‚¬ìš© ë¶ˆê°€ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë˜ëŠ” ì„¤ì • ì˜¤ë¥˜)")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("-" * 50)
    
    print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")

def test_provider_factory():
    """Provider Factory í…ŒìŠ¤íŠ¸"""
    print("\n=== Provider Factory í…ŒìŠ¤íŠ¸ ===")
    
    # Claude Provider í…ŒìŠ¤íŠ¸
    claude_config = {
        "provider": "claude",
        "model": "claude-sonnet-4-20250514"
    }
    claude_provider = LLMProviderFactory.create_provider(claude_config)
    print(f"Claude Provider: {claude_provider is not None}")
    if claude_provider:
        print(f"  - ì‚¬ìš© ê°€ëŠ¥: {claude_provider.is_available()}")
    
    # Ollama Provider í…ŒìŠ¤íŠ¸
    ollama_config = {
        "provider": "ollama",
        "model": "llama3.2",
        "base_url": "http://localhost:11434"
    }
    ollama_provider = LLMProviderFactory.create_provider(ollama_config)
    print(f"Ollama Provider: {ollama_provider is not None}")
    if ollama_provider:
        print(f"  - ì‚¬ìš© ê°€ëŠ¥: {ollama_provider.is_available()}")
    
    # ê¸°ë³¸ Provider í…ŒìŠ¤íŠ¸
    default_provider = LLMProviderFactory.get_default_provider()
    print(f"Default Provider: {default_provider is not None}")
    if default_provider:
        print(f"  - ì‚¬ìš© ê°€ëŠ¥: {default_provider.is_available()}")

def test_structured_output():
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ í…ŒìŠ¤íŠ¸ (Pydantic format)"""
    print("\n=== êµ¬ì¡°í™”ëœ ì¶œë ¥ í…ŒìŠ¤íŠ¸ ===")
    
    # ê³„ì • ì •ë³´ ë¡œë“œ
    accounts = load_accounts()
    
    for set_name, account_info in accounts.items():
        llm_config = account_info.get('llm', {})
        provider_type = llm_config.get('provider', 'claude').lower()
        
        print(f"\n--- {set_name} ({provider_type}) ---")
        
        try:
            llm_provider = get_llm_provider(set_name, accounts)
            
            if llm_provider.is_available():
                # ì£¼ì œ ì„ ì • êµ¬ì¡°í™” í…ŒìŠ¤íŠ¸
                print("1. ì£¼ì œ ì„ ì • êµ¬ì¡°í™” ì¶œë ¥ í…ŒìŠ¤íŠ¸")
                topic_messages = [
                    {
                        "role": "user",
                        "content": "ë‹¤ìŒ ì£¼ì œë“¤ ì¤‘ 3ê°œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:\n1. AI ê¸°ìˆ  ë™í–¥\n2. ê±´ê°•í•œ ì‹ë‹¨\n3. ë¶€ë™ì‚° íˆ¬ì\n4. ì—¬í–‰ íŒ\n5. ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…\n\nì •í™•íˆ 3ê°œì˜ ë²ˆí˜¸ë§Œ ì„ íƒí•˜ì„¸ìš”."
                    }
                ]
                
                response = llm_provider.generate(
                    messages=topic_messages,
                    system_prompt="ì£¼ì œë¥¼ ì„ ì •í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                    max_tokens=200,
                    temperature=0,
                    format=TopicSelection
                )
                
                if response:
                    print(f"âœ… ì£¼ì œ ì„ ì • ì‘ë‹µ: {response[:200]}...")
                    
                    # Pydantic íŒŒì‹± í…ŒìŠ¤íŠ¸
                    try:
                        topic_selection = TopicSelection.model_validate_json(response)
                        print(f"ğŸ“‹ ì„ ì •ëœ ì£¼ì œ ë²ˆí˜¸: {topic_selection.selected_numbers}")
                        print(f"ğŸ’­ ì„ ì • ì´ìœ : {topic_selection.reasoning}")
                    except Exception as e:
                        print(f"âš ï¸ Pydantic íŒŒì‹± ì‹¤íŒ¨: {e}")
                else:
                    print("âŒ ì£¼ì œ ì„ ì • ì‘ë‹µ ì‹¤íŒ¨")
                
                print()
                
                # ë¸”ë¡œê·¸ ì½˜í…ì¸  êµ¬ì¡°í™” í…ŒìŠ¤íŠ¸
                print("2. ë¸”ë¡œê·¸ ì½˜í…ì¸  êµ¬ì¡°í™” ì¶œë ¥ í…ŒìŠ¤íŠ¸")
                blog_messages = [
                    {
                        "role": "user", 
                        "content": "AI ê¸°ìˆ  ë™í–¥ì— ëŒ€í•œ ì§§ì€ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì œëª©, ë‚´ìš©, ì¹´í…Œê³ ë¦¬, íƒœê·¸ë¥¼ í¬í•¨í•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
                    }
                ]
                
                response = llm_provider.generate(
                    messages=blog_messages,
                    system_prompt="ë¸”ë¡œê·¸ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                    max_tokens=800,
                    temperature=0,
                    format=BlogContentResponse
                )
                
                if response:
                    print(f"âœ… ë¸”ë¡œê·¸ ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì")
                    
                    # Pydantic íŒŒì‹± í…ŒìŠ¤íŠ¸
                    try:
                        blog_content = BlogContentResponse.model_validate_json(response)
                        print(f"ğŸ“° ì œëª©: {blog_content.title}")
                        print(f"ğŸ“ ë‚´ìš© ê¸¸ì´: {len(blog_content.content)} ë¬¸ì")
                        print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {blog_content.category}")
                        print(f"ğŸ·ï¸ íƒœê·¸: {blog_content.tags}")
                    except Exception as e:
                        print(f"âš ï¸ Pydantic íŒŒì‹± ì‹¤íŒ¨: {e}")
                        # ì›ë³¸ ì‘ë‹µ ì¼ë¶€ ì¶œë ¥
                        print(f"ì›ë³¸ ì‘ë‹µ: {response[:300]}...")
                else:
                    print("âŒ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì‘ë‹µ ì‹¤íŒ¨")
                    
            else:
                print("âš ï¸ Provider ì‚¬ìš© ë¶ˆê°€")
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("-" * 50)

if __name__ == "__main__":
    test_provider_factory()
    test_llm_providers()
    test_structured_output()